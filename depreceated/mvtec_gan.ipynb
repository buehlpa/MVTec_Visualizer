{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate clip env\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torch.utils.tensorboard \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*pandas only supports SQLAlchemy connectable.*\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(f'GPU is available: {torch.cuda.is_available()}')\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHs\n",
    "PROJECT_DATA_PATH='//home/bule/projects/MVTec_Visualizer/data/mvtec_anomaly_detection'\n",
    "LOG_DIR= '/home/bule/projects/MVTec_Visualizer/tensorboard_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'MVTEC_embeddings_df'\n",
    "df = pd.read_pickle(os.path.join(PROJECT_DATA_PATH,FILENAME+'.pkl'))\n",
    "\n",
    "## Uncomment to filter for category\n",
    "# category = 'bottle' # categories = ['bottle', 'cable', 'capsule', 'carpet', 'grid', 'hazelnut', 'leather', 'metal_nut', 'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper']\n",
    "# # filter for category \n",
    "# df = df[df.index.str.contains(category)]\n",
    "# df = df[df.index.str.contains('good')]\n",
    "\n",
    "image_paths=[pts[1:]for pts in df.index]\n",
    "df = df.set_index(pd.Index(image_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train\n",
    "num_epochs = 20  # Set the number of epochs\n",
    "\n",
    "# Check for GPU and set the default device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generator definition\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, embedding_dim, img_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_shape = img_shape\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, torch.prod(torch.tensor(img_shape)).item()),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img\n",
    "\n",
    "# Discriminator definition\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(torch.prod(torch.tensor(img_shape))), 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        flattened = img.view(img.size(0), -1)\n",
    "        validity = self.model(flattened)\n",
    "        return validity\n",
    "\n",
    "# Custom dataset definition\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, embeddings_df, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.embeddings_df = embeddings_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.image_paths[index]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        embedding = self.embeddings_df.loc[img_path].values.astype('float32')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image.to(device), torch.tensor(embedding).to(device)\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Assuming 'image_paths' is a list of file paths and 'df' is your DataFrame with embeddings\n",
    "# image_paths = [...]\n",
    "# df = pd.DataFrame(...)\n",
    "\n",
    "# Initialize generator and discriminator with the correct device\n",
    "img_shape = (3, 64, 64)  # Example image shape\n",
    "embedding_dim = 512  # Example embedding size from CLIP\n",
    "generator = Generator(embedding_dim=embedding_dim, img_shape=img_shape).to(device)\n",
    "discriminator = Discriminator(img_shape=img_shape).to(device)\n",
    "\n",
    "# Loss function and optimizers\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Dataset and Dataloader\n",
    "dataset = CustomDataset(image_paths, df, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, embeddings) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(imgs.size(0), 1, device=device)\n",
    "        fake = torch.zeros(imgs.size(0), 1, device=device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(embeddings)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, num_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "        )\n",
    "\n",
    "# Save the models\n",
    "torch.save(generator.state_dict(), 'generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate from training samples\n",
    "\n",
    "# Load the generator from the given path\n",
    "generator_path = '/home/bule/projects/MVTec_Visualizer/generator.pth'\n",
    "generator = Generator(embedding_dim=embedding_dim, img_shape=img_shape).to(device)\n",
    "generator.load_state_dict(torch.load(generator_path))\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to denormalize image for visualization\n",
    "def denormalize(tensor):\n",
    "    return tensor * 0.5 + 0.5  # Assuming mean=0.5, std=0.5 for normalization\n",
    "\n",
    "# Function to display images\n",
    "def show_images(original, reconstructed):\n",
    "    num_images = len(original)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Display original image\n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        image = denormalize(original[i]).permute(1, 2, 0).cpu().numpy()\n",
    "        plt.imshow(image)\n",
    "        plt.title('Original')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Display reconstructed image\n",
    "        plt.subplot(2, num_images, num_images + i + 1)\n",
    "        image = denormalize(reconstructed[i]).permute(1, 2, 0).cpu().numpy()\n",
    "        plt.imshow(image)\n",
    "        plt.title('Reconstructed')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'generator' is your trained generator model\n",
    "# Assuming 'df' is your dataframe where the index is image paths and the columns are the embeddings\n",
    "generator.eval()\n",
    "\n",
    "# Number of images to sample\n",
    "num_samples = 5\n",
    "\n",
    "# Sample random embeddings and their corresponding image paths\n",
    "sample_indices = np.random.choice(df.index, size=num_samples, replace=False)\n",
    "sample_embeddings = torch.stack([torch.tensor(df.loc[idx].values).float() for idx in sample_indices]).to(device)\n",
    "sample_images_paths = [path for path in sample_indices]\n",
    "\n",
    "# Transform for the images (resize and normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize to the size expected by the generator\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load and transform the original images\n",
    "original_images = [transform(Image.open(path).convert('RGB')) for path in sample_images_paths]\n",
    "original_images = torch.stack(original_images).to(device)\n",
    "\n",
    "# Generate the reconstructed images from the embeddings\n",
    "with torch.no_grad():\n",
    "    reconstructed_images = generator(sample_embeddings).to(device)\n",
    "\n",
    "# Show original and reconstructed images\n",
    "show_images(original_images, reconstructed_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "\n",
    "FILENAME = 'MVTEC_embeddings_df'\n",
    "df = pd.read_pickle(os.path.join(PROJECT_DATA_PATH,FILENAME+'.pkl'))\n",
    "\n",
    "category = 'capsule' # categories = ['bottle', 'cable', 'capsule', 'carpet', 'grid', 'hazelnut', 'leather', 'metal_nut', 'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper']\n",
    "# filter for category \n",
    "df = df[df.index.str.contains(category)]\n",
    "df = df[df.index.str.contains('good')]\n",
    "\n",
    "image_paths=[pts[1:]for pts in df.index]\n",
    "df = df.set_index(pd.Index(image_paths))\n",
    "\n",
    "\n",
    "# Number of images to sample\n",
    "num_samples = 5\n",
    "\n",
    "# Sample random embeddings and their corresponding image paths\n",
    "sample_indices = np.random.choice(df.index, size=num_samples, replace=False)\n",
    "sample_embeddings = torch.stack([torch.tensor(df.loc[idx].values).float() for idx in sample_indices]).to(device)\n",
    "sample_images_paths = [path for path in sample_indices]\n",
    "\n",
    "# Transform for the images (resize and normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize to the size expected by the generator\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load and transform the original images\n",
    "original_images = [transform(Image.open(path).convert('RGB')) for path in sample_images_paths]\n",
    "original_images = torch.stack(original_images).to(device)\n",
    "\n",
    "# Generate the reconstructed images from the embeddings\n",
    "with torch.no_grad():\n",
    "    reconstructed_images = generator(sample_embeddings).to(device)\n",
    "\n",
    "# Show original and reconstructed images\n",
    "show_images(original_images, reconstructed_images)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
