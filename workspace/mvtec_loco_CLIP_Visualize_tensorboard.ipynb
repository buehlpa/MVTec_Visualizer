{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVTEc  LOCO  Visualizer\n",
    "\n",
    "- claculates clip embeddings\n",
    "- vizualisation via tensorboard projector\n",
    "\n",
    "from: \"https://www.mydrive.ch/shares/48237/1b9106ccdfbb09a0c414bd49fe44a14a/download/430647091-1646842701/mvtec_loco_anomaly_detection.tar.xz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: True\n",
      "Torch version: 2.1.0+cu118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 16:09:01.759098: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-12 16:09:02.041679: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-12 16:09:02.041695: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-12 16:09:02.043123: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-12 16:09:02.177281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-12 16:09:02.712069: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# activate clip env\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*pandas only supports SQLAlchemy connectable.*\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(f'GPU is available: {torch.cuda.is_available()}')\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision\n",
    "import torch.utils.tensorboard \n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_images_for_tensorboard(image_paths):\n",
    "    # Preprocess the images\n",
    "    transform_display = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(64),  # Resize the images \n",
    "        torchvision.transforms.CenterCrop(64),  # Crop the images\n",
    "        torchvision.transforms.ToTensor()  # Convert images to tensors        \n",
    "    ])\n",
    "\n",
    "    images = []  # Array to store the preprocessed images\n",
    "    for path in image_paths:\n",
    "        image = Image.open(path).convert('RGB')  # Open the image and convert to RGB\n",
    "        # Apply the display transformation\n",
    "        image_display = transform_display(image)\n",
    "        image_display = image_display.unsqueeze(0) # Add a batch dimension to the image\n",
    "        images.append(image_display)\n",
    "\n",
    "    images = torch.cat(images, dim=0)  # Concatenate the images along the batch dimension\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHs\n",
    "PROJECT_DATA_PATH='/home/bule/projects/MVTec_visualizer/data/mvtec_loco'\n",
    "LOG_DIR= '/home/bule/projects/MVTec_visualizer/tensorboard_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paths to images\n",
    "image_paths = []\n",
    "for root, dirs, files in os.walk(PROJECT_DATA_PATH):\n",
    "    if 'ground_truth' not in root:\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "len(image_paths)\n",
    "## TODO embedd all images from MVTEC in CLIP space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/home/bule/projects/MVTec_visualizer/data/mvtec_loco'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/bule/projects/MVTec_Visualizer/mvtec_loco_Visualize.ipynb Cell 6\u001b[0m line \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/MVTec_Visualizer/mvtec_loco_Visualize.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# Convert all embeddings to DataFrame and save\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/MVTec_Visualizer/mvtec_loco_Visualize.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(all_embeddings, index\u001b[39m=\u001b[39mimage_paths)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/MVTec_Visualizer/mvtec_loco_Visualize.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     df\u001b[39m.\u001b[39;49mto_pickle(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(PROJECT_DATA_PATH, FILENAME \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/MVTec_Visualizer/mvtec_loco_Visualize.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bclt-dsk-t-7201/home/bule/projects/MVTec_Visualizer/mvtec_loco_Visualize.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(PROJECT_DATA_PATH,FILENAME\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.pkl\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/clip/lib/python3.9/site-packages/pandas/core/generic.py:3085\u001b[0m, in \u001b[0;36mNDFrame.to_pickle\u001b[0;34m(self, path, compression, protocol, storage_options)\u001b[0m\n\u001b[1;32m   3033\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \u001b[39mPickle (serialize) object to file.\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[39m4    4    9\u001b[39;00m\n\u001b[1;32m   3082\u001b[0m \u001b[39m\"\"\"\u001b[39;00m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m   3083\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpickle\u001b[39;00m \u001b[39mimport\u001b[39;00m to_pickle\n\u001b[0;32m-> 3085\u001b[0m to_pickle(\n\u001b[1;32m   3086\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   3087\u001b[0m     path,\n\u001b[1;32m   3088\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3089\u001b[0m     protocol\u001b[39m=\u001b[39;49mprotocol,\n\u001b[1;32m   3090\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   3091\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/clip/lib/python3.9/site-packages/pandas/io/pickle.py:105\u001b[0m, in \u001b[0;36mto_pickle\u001b[0;34m(obj, filepath_or_buffer, compression, protocol, storage_options)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    103\u001b[0m     protocol \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mHIGHEST_PROTOCOL\n\u001b[0;32m--> 105\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    106\u001b[0m     filepath_or_buffer,\n\u001b[1;32m    107\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    108\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    109\u001b[0m     is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    110\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    111\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# letting pickle write directly to the buffer is more memory-efficient\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     pickle\u001b[39m.\u001b[39mdump(obj, handles\u001b[39m.\u001b[39mhandle, protocol\u001b[39m=\u001b[39mprotocol)\n",
      "File \u001b[0;32m~/anaconda3/envs/clip/lib/python3.9/site-packages/pandas/io/common.py:739\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[0;32m--> 739\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[1;32m    741\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[1;32m    742\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    743\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/clip/lib/python3.9/site-packages/pandas/io/common.py:604\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    602\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[1;32m    603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[0;32m--> 604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/home/bule/projects/MVTec_visualizer/data/mvtec_loco'"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024  # Adjust this based on your GPU memory\n",
    "FILENAME = 'MVTEC_LOCO_embeddings_df'\n",
    "\n",
    "if not os.path.exists(os.path.join(PROJECT_DATA_PATH,FILENAME+'.pkl')):\n",
    "\n",
    "    # Load the model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, transform = clip.load('ViT-B/32', device=device)\n",
    "\n",
    "\n",
    "    # Function to get embeddings for a batch of images\n",
    "    def get_batch_embeddings(batch_paths):\n",
    "        images = [Image.open(p) for p in batch_paths]\n",
    "        tensors = [transform(img) for img in images]\n",
    "        batch_tensor = torch.stack(tensors).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_features = model.encode_image(batch_tensor)\n",
    "        return batch_features.cpu().numpy()\n",
    "\n",
    "    # Calculate embeddings for all images in batches\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(image_paths), BATCH_SIZE):\n",
    "        batch_paths = image_paths[i:i+BATCH_SIZE]\n",
    "        batch_embeddings = get_batch_embeddings(batch_paths)\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "        print(f\"Processed images {i} to {i + len(batch_embeddings)}\")\n",
    "\n",
    "    # Convert all embeddings to DataFrame and save\n",
    "    df = pd.DataFrame(all_embeddings, index=image_paths)\n",
    "    df.to_pickle(os.path.join(PROJECT_DATA_PATH, FILENAME + '.pkl'))\n",
    "else:\n",
    "    df = pd.read_pickle(os.path.join(PROJECT_DATA_PATH,FILENAME+'.pkl'))\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVTEC_LOCO_embeddings_df\n",
      "MVTEC_LOCO_embeddings_df  number of images:3651\n"
     ]
    }
   ],
   "source": [
    "# make tensorboard logs\n",
    "\n",
    "print(FILENAME)\n",
    "TENSORBOARD_LOGS_PATH=os.path.join(LOG_DIR,'_'+FILENAME).replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "if not  os.path.exists(TENSORBOARD_LOGS_PATH):\n",
    "\n",
    "    df = pd.read_pickle(os.path.join(PROJECT_DATA_PATH,FILENAME+'.pkl'))\n",
    "\n",
    "    # Convert the resulting series of lists to a NumPy array\n",
    "    numpy_array = df.to_numpy()\n",
    "\n",
    "    # resize images \n",
    "    images = convert_images_for_tensorboard(image_paths)\n",
    "\n",
    "    # for tensorboard\n",
    "    writer = SummaryWriter(TENSORBOARD_LOGS_PATH)\n",
    "    writer.add_embedding(numpy_array, label_img=images)\n",
    "\n",
    "else:\n",
    "    df = pd.read_pickle(os.path.join(PROJECT_DATA_PATH,FILENAME+'.pkl'))\n",
    "\n",
    "print(f'{FILENAME}  number of images:{len(image_paths)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run tensorboard on port forward to browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-20 13:07:39.932508: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-20 13:07:39.932545: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-20 13:07:39.932588: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-20 13:07:39.944385: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-20 13:07:41.337600: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-10-20 13:07:42.704029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-20 13:07:42.761861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-20 13:07:42.762120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.14.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!tensorboard  --logdir $TENSORBOARD_LOGS_PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformalyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
